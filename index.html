<!DOCTYPE html>
<html>
<head>
    <title>IEEE Vis 2024 LLM4Vis tutorial</title>
    <!-- CSS only -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.8.1/font/bootstrap-icons.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <style>
        .thumbnail { max-width: 100%; max-height: 100%; }
        .thumbnail-wrapper {
            width: 200px; height: 200px; display: block; margin: auto; padding-bottom: 20px;
        }
        i { margin-right: 20px; }
    </style>
</head>
<body data-new-gr-c-s-check-loaded="9.54.0" data-gr-ext-installed="">
    <main class="container">
        <div class="bg-light p-5 rounded mt-3">
            <h1 class="text-center">MLLM4Vis: Multimodal Large Language Models for Information Visualization</h1>
            <h4 class="text-center">Half-day tutorial at <a href="https://ieeevis.org/year/2025/info/program/tutorials#LLM">IEEE
                    Vis Conference 2024</a>.</h4>
        </div>

        <div class="bg-light p-5 rounded mt-3">
            <h2 class="text-center">Overview</h2>
            <p class="lead">
This tutorial will introd language-centric AI models to researchers in the visualization (Vis) community, with a special focus on Multimodal Large Language Models (MLLMs). We will begin by motivating why MLLM4Vis is a timely and important area of research, and how recent advances in MLLMs can be leveraged to support and enhance visualization tasks. The tutorial will cover the basics of language and vision-language models, including the Transformer architecture, key training paradigms, and techniques such as pre-training, fine-tuning, and prompt engineering. We will then explore the growing ecosystem of MLLMs (e.g., GPT-4V, Claude 3, and LLaVA), highlighting their abilities to reason over both visual and textual modalities. In the latter half, we will focus on practical applications of MLLMs in visualization, including visual text analytics, natural language interfaces for charts, chart question answering, text-to-visualization generation, automated visual storytelling, and improving accessibility in data communication. We will conclude with a forward-looking discussion of the research opportunities and open challenges in this emerging area. This tutorial is designed for visualization researchers interested in multimodal AI as well as those broadly curious about integrating MLLMs into visual analytic systems.


            </p>
        </div>

        <div class="bg-light p-5 rounded mt-3">
            <h2 class="text-center">Materials</h2>
            <div class="container">
                <div class="row">
                    <h3>Tutorial Overview</h3>

                    <h4 style="font-size: 1.1em;">Part 1: Introduction [15 min]</h4>
                    <ul style="margin-left: 20px;">
                        <li>Why combine NLP and visualization?</li>
                        <li>Evolution: From LM → LLMs → MLLMs.</li>
                        <li>Overview of MLLM+Vis research landscape.</li>
                        <li>Tutorial roadmap and objectives.</li>
                    </ul>

                    <h4 style="font-size: 1.1em;">Part 2: Foundations of MLLMs [60 min]</h4>
                    <ul style="margin-left: 20px;">
                        <li>Basics of language models
                            <ul>
                                <li>Language modeling</li>
                                <li>Model architectures
                                    <ul>
                                        <li>Transformer architecture</li>
                                        <li>Encoder, decoder, encoder–decoder</li>
                                    </ul>
                                </li>
                                <li>Model training
                                    <ul>
                                        <li>Pre-training</li>
                                        <li>Fine-tuning</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>Large language models (LLMs)
                            <ul>
                                <li>Scaling LMs to LLMs</li>
                                <li>Prompt engineering
                                    <ul>
                                        <li>In-context learning</li>
                                        <li>Chain-of-thought prompts</li>
                                        <li>Program-aided language models</li>
                                        <li>ReAct: Reasoning + Action</li>
                                    </ul>
                                </li>
                                <li>Multi-agent systems</li>
                                <li>Multimodal LLMs</li>
                            </ul>
                        </li>
                    </ul>

                    <h4 style="font-size: 1.1em; color: #333;">Coffee Break &nbsp;<i class="fas fa-coffee"></i></h4>

                    <h4 style="font-size: 1.1em;">Part 3: Applications [60 min]</h4>
                    <ul style="margin-left: 20px;">
                        <li>Natural language interfaces (NLI)
                            <ul>
                                <li>Chart question answering (CQA)</li>
                                <li>Conversational and multimodal interfaces</li>
                            </ul>
                        </li>
                        <li>Chart captioning (Vis2Text)</li>
                        <li>Visualization generation and editing (Text2Vis)</li>
                        <li>Visual data story generation</li>
                        <li>Accessibility and educational use cases</li>
                    </ul>

                    <h4 style="font-size: 1.1em;">Part 4: Future Challenges [20 min]</h4>
                    <ul style="margin-left: 20px;">
                        <li>Benchmarks and evaluation</li>
                        <li>Ethics, bias, fairness, and hallucination</li>
                        <li>Responsible and inclusive visualization practices</li>
                        <li>Human-AI collaboration in analytic workflows</li>
                        <li>Emerging research directions and interdisciplinary opportunities</li>
                    </ul>

                    <h3>Slides</h3>
                    <p><a href="https://www.dropbox.com/scl/fi/4wpled6lrbgck3vn21whn/LLM4Vis-Intro.pdf?rlkey=gbd0lg7r3w5atj6nirdu7auj7&dl=0">Part 1: Introduction</a></p>
                    <p><a href="https://www.dropbox.com/scl/fi/mex4eep16hzupn4oiml5p/LLM4Vis-LM.pdf?rlkey=fob3sk55cc0tn9jvnwuk5czlm&dl=0">Part 2: LLMs for Visualizations</a></p>
                    <p><a href="https://www.dropbox.com/scl/fi/3hgw3rz3bgs6n4n039dco/MLLM4Vis-Application-Future.pdf?rlkey=06rmh1d6mu70ldzj59b5h7g23&dl=0">LLM4Vis applications, Future Challenges</a></p>
                </div>
            </div>
        </div>

        <div class="bg-light p-5 rounded mt-3">
            <h2 class="text-center">Organizer</h2>
            <div class="container">
                <div class="row">
                    <div class="col-sm text-center">
                        <div class="thumbnail-wrapper">
                            <img src="https://www.yorku.ca/enamulh/images/me2017.jpg" alt="Enamul Hoque"
                                class="img-thumbnail thumbnail">
                        </div>
                        <h5><a href="https://www.yorku.ca/enamulh/">Enamul Hoque</a></h5>
                        <p>Associate Professor, York University</p>
                        <a href="mailto:enamulh@yorku.ca"><i class="bi bi-envelope" style="font-size: 2rem;"></i></a>
                        <a href="https://twitter.com/Enamul_Hoque"><i class="bi bi-twitter" style="font-size: 2rem;"></i></a>
                    </div>
                </div>
            </div>
        </div>

        <div class="bg-light p-5 rounded mt-3">
            <h2 class="text-center">Previous Versions of the Tutorial</h2>
            <p class="lead"></p>
            Related tutorials were presented at the
            <a href="IEEEVis-2024/index.html" class="link">IEEE Vis 2024</a>, <a href="IEEEVis-2023/index.html" class="link">IEEE Vis 2023</a> and <a href="EMNLP-2023/index.html"
                class="link">EMNLP 2023</a>
        </div>
    </main>

    <script src="/docs/5.1/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p"
        crossorigin="anonymous"></script>
</body>
</html>
